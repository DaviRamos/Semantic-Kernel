name: ProjetoSemanticKernel

# É necessário baixar este modelo sno container do ollama
# ollama pull mxbai-embed-large:latest
# ollama pull llama3.2:latest


networks:
  local-network:
    external: true

volumes:
  ollama-data:  

services: 
  ollama:
    image: ollama/ollama
    container_name: ollamaSK-container
#    command: ["ollama", "pull", "llama2"]
    networks:
      - local-network
    volumes:
      - ollama-data:/root/.ollama    
    ports:
      - "11434:11434"
    pull_policy: always
    tty: true
    restart: always
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.50'  
          devices:
            - driver: ${OLLAMA_GPU_DRIVER-nvidia}
              count: ${OLLAMA_GPU_COUNT-1}
              capabilities:
                - gpu  